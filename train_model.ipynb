{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "# !pip install tqdm\n",
    "# !pip install feature-engine\n",
    "# !pip install scikit-learn\n",
    "# !pip install lightgbm\n",
    "# !pip install --upgrade scikit-learn\n",
    "# !pip install --upgrade <cyclical-features-library>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Data\n",
    "\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "-> Clean the data by adding the mean value to the fields where it is missing or a random value where the mean can t be calculated\n",
    "-> dropping the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_age = train['Age'].mean()\n",
    "train['Age'] = train['Age'].fillna(mean_age)\n",
    "test['Age'] = test['Age'].fillna(mean_age)\n",
    "\n",
    "median_annual_income = train['Annual Income'].median()\n",
    "train['Annual Income'] = train['Annual Income'].fillna(median_annual_income)\n",
    "test['Annual Income'] = test['Annual Income'].fillna(median_annual_income)\n",
    "\n",
    "train = train.dropna(subset=['Marital Status'])\n",
    "test['Marital Status'] = test['Marital Status'].apply(\n",
    "    lambda x: np.random.choice(['Single', 'Married', 'Divorced'],\n",
    "                               p=[0.3347, 0.3337, 0.3316]) if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "mean_number_dependents = train['Number of Dependents'].mean()\n",
    "train['Number of Dependents'] = train['Number of Dependents'].fillna(mean_number_dependents)\n",
    "test['Number of Dependents'] = test['Number of Dependents'].fillna(mean_number_dependents)\n",
    "\n",
    "train['Occupation'] = train['Occupation'].fillna('Missing')\n",
    "test['Occupation'] = test['Occupation'].fillna('Missing')\n",
    "\n",
    "mean_health_score = train['Health Score'].mean()\n",
    "train['Health Score'] = train['Health Score'].fillna(mean_health_score)\n",
    "test['Health Score'] = test['Health Score'].fillna(mean_health_score)\n",
    "\n",
    "data_vc_train = train.dropna(subset=['Vehicle Age'])\n",
    "mean_vehicle_age = test['Vehicle Age'].mean()\n",
    "test['Vehicle Age'] = test['Vehicle Age'].fillna(mean_vehicle_age)\n",
    "\n",
    "mean_credit_score = train['Credit Score'].mean()\n",
    "train['Credit Score'] = train['Credit Score'].fillna(mean_credit_score)\n",
    "test['Credit Score'] = test['Credit Score'].fillna(mean_credit_score)\n",
    "\n",
    "mean_insurance_duration = train['Insurance Duration'].mean()\n",
    "train['Insurance Duration'] = train['Insurance Duration'].fillna(mean_insurance_duration)\n",
    "test['Insurance Duration'] = test['Insurance Duration'].fillna(mean_insurance_duration)\n",
    "\n",
    "train['Customer Feedback'] = train['Customer Feedback'].fillna('Unknown')\n",
    "test['Customer Feedback'] = test['Customer Feedback'].fillna('Unknown')\n",
    "\n",
    "train.drop(columns=['id'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cycling dates\n",
    "train['Policy Start Date'] = pd.to_datetime(train['Policy Start Date'])\n",
    "test['Policy Start Date'] = pd.to_datetime(test['Policy Start Date'])\n",
    "\n",
    "# Cyclical encoding of the year - no need for cos and sinus interpretations\n",
    "train['Year'] = train['Policy Start Date'].dt.year\n",
    "test['Year'] = test['Policy Start Date'].dt.year\n",
    "\n",
    "train['Month'] = train['Policy Start Date'].dt.month\n",
    "test['Month'] = test['Policy Start Date'].dt.month\n",
    "\n",
    "train['Month_cos'] = np.cos(2 * np.pi * train['Month'] / 12)\n",
    "train['Month_sin'] = np.sin(2 * np.pi * train['Month'] / 12)\n",
    "\n",
    "test['Month_cos'] = np.cos(2 * np.pi * test['Month'] / 12)\n",
    "test['Month_sin'] = np.sin(2 * np.pi * test['Month'] / 12)\n",
    "\n",
    "train['Day'] = train['Policy Start Date'].dt.day\n",
    "test['Day'] = test['Policy Start Date'].dt.day\n",
    "\n",
    "train['Day_cos'] = np.cos(2 * np.pi * train['Day'] / 30)\n",
    "train['Day_sin'] = np.sin(2 * np.pi * train['Day'] / 30)\n",
    "\n",
    "test['Day_cos'] = np.cos(2 * np.pi * test['Day'] / 30)\n",
    "test['Day_sin'] = np.sin(2 * np.pi * test['Day'] / 30)\n",
    "\n",
    "train['Group'] = (train['Year'] - 2020) * 48 + train['Month'] * 4 + train['Day'] // 7\n",
    "test['Group'] = (test['Year'] - 2020) * 48 + test['Month'] * 4 + test['Day'] // 7\n",
    "\n",
    "train.drop(columns=['Day','Month','Policy Start Date'], inplace=True)\n",
    "test.drop(columns=['Day','Month','Policy Start Date' ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the categorical columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = le.fit_transform(train[col])\n",
    "        test[col] = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert categorical columns to category dtype for Catboost\n",
    "# categorical_columns = ['Gender', 'Marital Status', 'Education Level', 'Occupation', \n",
    "#                        'Location', 'Policy Type', 'Previous Claims', 'Smoking Status', \n",
    "#                        'Exercise Frequency', 'Property Type', 'Customer Feedback']\n",
    "\n",
    "# for col in categorical_columns:\n",
    "#     test[col] = test[col].astype('category')\n",
    "#     train[col] = train[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Premium Amount']\n",
    "X = train.drop(columns=['Premium Amount'])\n",
    "\n",
    "X_train, X_ref, y_train, y_ref = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsLe(y_true, y_pred):\n",
    "    y_pred = np.maximum(y_pred, 1e-6)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# SEED = 42\n",
    "# n_splits = 5\n",
    "\n",
    "# def TrainML(params, e_stop=50):\n",
    "#     kfold = RepeatedKFold(n_splits=n_splits, n_repeats=1, random_state=SEED)\n",
    "#     train_rmse_scores = []\n",
    "#     val_rmse_scores = []\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "#         X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#         y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#         y_train_log = np.log1p(y_train)\n",
    "#         y_val_log = np.log1p(y_val)\n",
    "        \n",
    "#         callbacks = [lgb.early_stopping(stopping_rounds=e_stop, verbose=False)]\n",
    "#         model = lgb.LGBMRegressor(**params, random_state=SEED, verbose=-1, n_jobs=-1)\n",
    "#         model.fit(X_train, y_train_log, \n",
    "#                   eval_set=[(X_val, y_val_log)], \n",
    "#                   eval_metric='rmse', \n",
    "#                   callbacks=callbacks)\n",
    "        \n",
    "#         y_train_log_pred = model.predict(X_train)\n",
    "#         y_val_log_pred = model.predict(X_val)\n",
    "        \n",
    "#         y_train_pred = np.expm1(y_train_log_pred)\n",
    "#         y_val_pred = np.expm1(y_val_log_pred)\n",
    "        \n",
    "#         train_rmse = rmsLe(y_train, y_train_pred)\n",
    "#         val_rmse = rmsLe(y_val, y_val_pred)\n",
    "\n",
    "#         train_rmse_scores.append(train_rmse)\n",
    "#         val_rmse_scores.append(val_rmse)\n",
    "\n",
    "#     mean_train_rmse = np.mean(train_rmse_scores)\n",
    "#     mean_val_rmse = np.mean(val_rmse_scores)\n",
    "\n",
    "#     print(\"\\n Final Mean Scores:\")\n",
    "#     print(f\" Mean Train RMSLE: {mean_train_rmse:.4f}\")\n",
    "#     print(f\" Mean Validation RMSLE: {mean_val_rmse:.4f}\")\n",
    "\n",
    "#     return mean_train_rmse, mean_val_rmse\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "#         'device': 'gpu'\n",
    "#     }\n",
    "\n",
    "#     mean_train_rmse, mean_val_rmse = TrainML(params)\n",
    "#     trial.set_user_attr('train_rmse', mean_train_rmse)\n",
    "#     trial.set_user_attr('val_rmse', mean_val_rmse)\n",
    "#     return mean_val_rmse\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# for col in categorical_columns:\n",
    "#     # Ensure the column exists in both datasets\n",
    "#     if col in X_train.columns and col in X_ref.columns:\n",
    "#         if X_train[col].dtype.name == \"category\":\n",
    "#             # Check if \"missing\" is already a category\n",
    "#             if \"missing\" not in X_train[col].cat.categories:\n",
    "#                 X_train[col] = X_train[col].cat.add_categories([\"missing\"])\n",
    "#                 X_ref[col] = X_ref[col].cat.add_categories([\"missing\"])\n",
    "        \n",
    "#         # Handle missing values and convert to string\n",
    "#         X_train[col] = X_train[col].fillna(\"missing\").astype(str)\n",
    "#         X_ref[col] = X_ref[col].fillna(\"missing\").astype(str)\n",
    "\n",
    "# # Process all categorical features to ensure consistency\n",
    "# for col in categorical_columns:\n",
    "#     X_train[col] = X_train[col].fillna(\"missing\").astype(str)\n",
    "#     X_ref[col] = X_ref[col].fillna(\"missing\").astype(str)\n",
    "\n",
    "# cat_features = categorical_columns\n",
    "\n",
    "# best_params_catboost = {'iterations': 50, 'learning_rate': 0.006002641114390338, 'depth': 15, 'l2_leaf_reg': 0.40630468090624405,\n",
    "#     'subsample': 0.7200344333898133, 'colsample_bylevel': 0.6986521753741868, 'min_data_in_leaf': 72, 'random_state': 42, \n",
    "#     'cat_features': categorical_columns, 'verbose': 200}\n",
    "\n",
    "# catboost_model = CatBoostRegressor(**best_params_catboost)\n",
    "\n",
    "# best_params = {'n_estimators': 700, 'learning_rate': 0.006002641114390338, 'num_leaves': 42, 'max_depth': 15, 'min_child_samples': 72, \n",
    "#                'subsample': 0.7200344333898133, 'colsample_bytree': 0.6986521753741868, 'reg_alpha': 0.0013223748424590466, \n",
    "#                'reg_lambda': 0.40630468090624405}\n",
    "\n",
    "# #Training LGBMRegressor Model - It is known for being fast and efficient in training, especially on large datasets.\n",
    "# LGBMR_model = LGBMRegressor(**best_params, verbose=-1, n_jobs=-1)\n",
    "# LGBMR_model.fit(X_train, np.log1p(y_train))#this log is recommended when the distribution is skewed\n",
    "\n",
    "# #Make predictions\n",
    "# test_preds = LGBMR_model.predict(X_ref)\n",
    "# test_preds = np.expm1(test_preds)\n",
    "\n",
    "# error_lgbm = rmsLe(y_ref, test_preds)\n",
    "\n",
    "# # sample_submission['Premium Amount'] = test_preds\n",
    "# # sample_submission.to_csv('submission1.csv', index=False)\n",
    "\n",
    "# #Visualising the importance of each field of the dataset in determing the Premium Amount\n",
    "# importances = LGBMR_model.feature_importances_\n",
    "# norm_importances = importances / np.linalg.norm(importances)\n",
    "# features = X.columns\n",
    "# importance_df = pd.DataFrame({'Feature': features, 'Importance': norm_importances})\n",
    "# importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "# plt.title('Error = {:.5f}'.format(error_lgbm))\n",
    "# plt.show()\n",
    "# Error = 1.05377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# best_params_xgb = {'n_estimators': 700, 'learning_rate': 0.006002641114390338, 'max_depth': 15, 'min_child_weight': 72,\n",
    "#                    'subsample': 0.7200344333898133, 'colsample_bytree': 0.6986521753741868, 'reg_alpha': 0.0013223748424590466,\n",
    "#                     'reg_lambda': 0.40630468090624405}\n",
    "\n",
    "# xgb_model = XGBRegressor(**best_params_xgb, objective='reg:squarederror', random_state=42)\n",
    "# xgb_model.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "# test_preds = xgb_model.predict(X_ref)\n",
    "# test_preds = np.expm1(test_preds)\n",
    "\n",
    "# error_xgb = rmsLe(y_ref, test_preds)\n",
    "\n",
    "# # sample_submission['Premium Amount'] = test_preds\n",
    "# # sample_submission.to_csv('submission1.csv', index=False)\n",
    "\n",
    "# #Visualising the importance of each field of the dataset in determing the Premium Amount\n",
    "# importances = xgb_model.feature_importances_\n",
    "# norm_importances = importances / np.linalg.norm(importances)\n",
    "# features = X.columns\n",
    "# importance_df = pd.DataFrame({'Feature': features, 'Importance': norm_importances})\n",
    "# importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "# plt.title('Error = {:.5f}'.format(error_xgb))\n",
    "# plt.show()\n",
    "# #Error = 1.05411"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy catboost\n",
    "# !pip install numpy catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "# for col in categorical_columns:\n",
    "#     # Ensure the column exists in both datasets\n",
    "#     if col in X_train.columns and col in X_ref.columns:\n",
    "#         if X_train[col].dtype.name == \"category\":\n",
    "#             # Check if \"missing\" is already a category\n",
    "#             if \"missing\" not in X_train[col].cat.categories:\n",
    "#                 X_train[col] = X_train[col].cat.add_categories([\"missing\"])\n",
    "#                 X_ref[col] = X_ref[col].cat.add_categories([\"missing\"])\n",
    "        \n",
    "#         # Handle missing values and convert to string\n",
    "#         X_train[col] = X_train[col].fillna(\"missing\").astype(str)\n",
    "#         X_ref[col] = X_ref[col].fillna(\"missing\").astype(str)\n",
    "\n",
    "# # Process all categorical features to ensure consistency\n",
    "# for col in categorical_columns:\n",
    "#     X_train[col] = X_train[col].fillna(\"missing\").astype(str)\n",
    "#     X_ref[col] = X_ref[col].fillna(\"missing\").astype(str)\n",
    "\n",
    "# cat_features = categorical_columns\n",
    "\n",
    "# best_params_catboost = {'iterations': 50, 'learning_rate': 0.006002641114390338, 'depth': 15, 'l2_leaf_reg': 0.40630468090624405,\n",
    "#     'subsample': 0.7200344333898133, 'colsample_bylevel': 0.6986521753741868, 'min_data_in_leaf': 72, 'random_state': 42, \n",
    "#     'cat_features': categorical_columns, 'verbose': 200}\n",
    "\n",
    "# catboost_model = CatBoostRegressor(**best_params_catboost)\n",
    "\n",
    "# catboost_model.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "# test_preds = catboost_model.predict(X_ref)\n",
    "# test_preds = np.expm1(test_preds)  # Transform predictions back from log space\n",
    "\n",
    "# # Convert predictions to a DataFrame\n",
    "# sample_submission = pd.DataFrame({'Premium Amount': test_preds})\n",
    "# sample_submission.to_csv('submissionCATBOOST.csv', index=False)\n",
    "\n",
    "# # error_catboost = rmsLe(y_ref, test_preds)\n",
    "\n",
    "# # #Visualising the importance of each field of the dataset in determing the Premium Amount\n",
    "# # importances = catboost_model.feature_importances_\n",
    "# # norm_importances = importances / np.linalg.norm(importances)\n",
    "# # features = X.columns\n",
    "# # importance_df = pd.DataFrame({'Feature': features, 'Importance': norm_importances})\n",
    "# # importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# # plt.figure(figsize=(12, 8))\n",
    "# # sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "# # plt.title('Error = {:.5f}'.format(error_catboost))\n",
    "# # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
