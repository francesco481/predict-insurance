{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "# !pip install tqdm\n",
    "# !pip install feature-engine\n",
    "# !pip install scikit-learn\n",
    "# !pip install lightgbm\n",
    "# !pip install --upgrade scikit-learn\n",
    "# !pip install --upgrade <cyclical-features-library>\n",
    "# !pip install torch torchvision\n",
    "# !pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "sample_submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "-> Clean the data by adding the mean value to the fields where it is missing or a random value where the mean can t be calculated\n",
    "-> dropping the id column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_age = train['Age'].mean()\n",
    "train['Age'] = train['Age'].fillna(mean_age)\n",
    "test['Age'] = test['Age'].fillna(mean_age)\n",
    "\n",
    "median_annual_income = train['Annual Income'].median()\n",
    "train['Annual Income'] = train['Annual Income'].fillna(median_annual_income)\n",
    "test['Annual Income'] = test['Annual Income'].fillna(median_annual_income)\n",
    "\n",
    "train = train.dropna(subset=['Marital Status'])\n",
    "test['Marital Status'] = test['Marital Status'].apply(\n",
    "    lambda x: np.random.choice(['Single', 'Married', 'Divorced'],\n",
    "                               p=[0.3347, 0.3337, 0.3316]) if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "mean_number_dependents = train['Number of Dependents'].mean()\n",
    "train['Number of Dependents'] = train['Number of Dependents'].fillna(mean_number_dependents)\n",
    "test['Number of Dependents'] = test['Number of Dependents'].fillna(mean_number_dependents)\n",
    "\n",
    "train['Occupation'] = train['Occupation'].fillna('Missing')\n",
    "test['Occupation'] = test['Occupation'].fillna('Missing')\n",
    "\n",
    "mean_health_score = train['Health Score'].mean()\n",
    "train['Health Score'] = train['Health Score'].fillna(mean_health_score)\n",
    "test['Health Score'] = test['Health Score'].fillna(mean_health_score)\n",
    "\n",
    "train = train.dropna(subset=['Vehicle Age'])\n",
    "mean_vehicle_age = test['Vehicle Age'].mean()\n",
    "test['Vehicle Age'] = test['Vehicle Age'].fillna(mean_vehicle_age)\n",
    "\n",
    "mean_credit_score = train['Credit Score'].mean()\n",
    "train['Credit Score'] = train['Credit Score'].fillna(mean_credit_score)\n",
    "test['Credit Score'] = test['Credit Score'].fillna(mean_credit_score)\n",
    "\n",
    "mean_insurance_duration = train['Insurance Duration'].mean()\n",
    "train['Insurance Duration'] = train['Insurance Duration'].fillna(mean_insurance_duration)\n",
    "test['Insurance Duration'] = test['Insurance Duration'].fillna(mean_insurance_duration)\n",
    "\n",
    "train['Customer Feedback'] = train['Customer Feedback'].fillna('Unknown')\n",
    "test['Customer Feedback'] = test['Customer Feedback'].fillna('Unknown')\n",
    "\n",
    "train.drop(columns=['id'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "train['Age'] = min_max_scaler.fit_transform(train[['Age']])\n",
    "test['Age'] = min_max_scaler.fit_transform(test[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "train['Annual Income'] = scaler.fit_transform(train[['Annual Income']])\n",
    "test['Annual Income'] = scaler.transform(test[['Annual Income']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train['Health Score'] = scaler.fit_transform(train[['Health Score']])\n",
    "test['Health Score'] = scaler.transform(test[['Health Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = train['Previous Claims'].quantile(0.25)\n",
    "Q3 = train['Previous Claims'].quantile(0.75)\n",
    "\n",
    "# Calculate the IQR (Interquartile Range)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to exclude outliers\n",
    "train = train[(train['Previous Claims'] >= lower_bound) & (train['Previous Claims'] <= upper_bound)]\n",
    "\n",
    "proportions = {\n",
    "    0.0: 0.365706,\n",
    "    1.0: 0.360090,\n",
    "    2.0: 0.200233,\n",
    "    3.0: 0.058445,\n",
    "    4.0: 0.012677,\n",
    "    5.0: 0.002410,\n",
    "    6.0: 0.000358,\n",
    "    7.0: 0.000070,\n",
    "    8.0: 0.000010,\n",
    "    9.0: 0.000001\n",
    "}\n",
    "\n",
    "categories = list(proportions.keys())\n",
    "probabilities = list(proportions.values())\n",
    "\n",
    "train['Previous Claims'] = train['Previous Claims'].apply(\n",
    "    lambda x: np.random.choice(categories, p=probabilities) if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "test['Previous Claims'] = test['Previous Claims'].apply(\n",
    "    lambda x: np.random.choice(categories, p=probabilities) if pd.isna(x) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Health Score</th>\n",
       "      <th>Location</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>Previous Claims</th>\n",
       "      <th>Vehicle Age</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>Insurance Duration</th>\n",
       "      <th>Policy Start Date</th>\n",
       "      <th>Customer Feedback</th>\n",
       "      <th>Smoking Status</th>\n",
       "      <th>Exercise Frequency</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Premium Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021739</td>\n",
       "      <td>Female</td>\n",
       "      <td>-0.393594</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>-0.254755</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-12-23 15:21:39.134960</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>House</td>\n",
       "      <td>2869.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456522</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.220534</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-0.849664</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-06-12 15:21:39.111551</td>\n",
       "      <td>Average</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>House</td>\n",
       "      <td>1483.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108696</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.048014</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>1.825495</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Premium</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>592.823151</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-09-30 15:21:39.221386</td>\n",
       "      <td>Good</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>House</td>\n",
       "      <td>567.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065217</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.348874</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-1.241663</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Basic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-06-12 15:21:39.226954</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>765.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.065217</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.446918</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>-0.442873</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Premium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>598.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-12-01 15:21:39.252145</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>House</td>\n",
       "      <td>2022.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
       "0  0.021739  Female      -0.393594        Married                   1.0   \n",
       "1  0.456522  Female       0.220534       Divorced                   3.0   \n",
       "2  0.108696    Male       0.048014       Divorced                   3.0   \n",
       "3  0.065217    Male       3.348874        Married                   2.0   \n",
       "4  0.065217    Male       0.446918         Single                   1.0   \n",
       "\n",
       "  Education Level     Occupation  Health Score  Location    Policy Type  \\\n",
       "0      Bachelor's  Self-Employed     -0.254755     Urban        Premium   \n",
       "1        Master's        Missing     -0.849664     Rural  Comprehensive   \n",
       "2     High School  Self-Employed      1.825495  Suburban        Premium   \n",
       "3      Bachelor's        Missing     -1.241663     Rural          Basic   \n",
       "4      Bachelor's  Self-Employed     -0.442873     Rural        Premium   \n",
       "\n",
       "   Previous Claims  Vehicle Age  Credit Score  Insurance Duration  \\\n",
       "0              2.0         17.0    372.000000                 5.0   \n",
       "1              1.0         12.0    694.000000                 2.0   \n",
       "2              1.0         14.0    592.823151                 3.0   \n",
       "3              1.0          0.0    367.000000                 1.0   \n",
       "4              0.0          8.0    598.000000                 4.0   \n",
       "\n",
       "            Policy Start Date Customer Feedback Smoking Status  \\\n",
       "0  2023-12-23 15:21:39.134960              Poor             No   \n",
       "1  2023-06-12 15:21:39.111551           Average            Yes   \n",
       "2  2023-09-30 15:21:39.221386              Good            Yes   \n",
       "3  2024-06-12 15:21:39.226954              Poor            Yes   \n",
       "4  2021-12-01 15:21:39.252145              Poor            Yes   \n",
       "\n",
       "  Exercise Frequency Property Type  Premium Amount  \n",
       "0             Weekly         House          2869.0  \n",
       "1            Monthly         House          1483.0  \n",
       "2             Weekly         House           567.0  \n",
       "3              Daily     Apartment           765.0  \n",
       "4             Weekly         House          2022.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1 = train['Premium Amount'].quantile(0.25)  # Calculate the first quartile (25th percentile) of the 'price' column\n",
    "Q3 = train['Premium Amount'].quantile(0.75)  # Calculate the third quartile (75th percentile) of the 'price' column\n",
    "IQR = Q3 - Q1  # Calculate the interquartile range (IQR) for 'price'\n",
    "\n",
    "# Define the boundaries to consider a value as an outlier\n",
    "lower_bound = Q1 - 1.5 * IQR  # Calculate the lower bound (below which values will be considered outliers)\n",
    "upper_bound = Q3 + 1.5 * IQR  # Calculate the upper bound (above which values will be considered outliers)\n",
    "\n",
    "# Filter out the outliers from the 'price' column\n",
    "train_df = train[(train['Premium Amount'] >= lower_bound) & (train['Premium Amount'] <= upper_bound)]  # Keep only the data points within the bounds\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add day_sin, day_cos, month_cos, month_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cycling dates\n",
    "train['Policy Start Date'] = pd.to_datetime(train['Policy Start Date'])\n",
    "test['Policy Start Date'] = pd.to_datetime(test['Policy Start Date'])\n",
    "\n",
    "# Cyclical encoding of the year - no need for cos and sinus interpretations\n",
    "train['Year'] = train['Policy Start Date'].dt.year\n",
    "test['Year'] = test['Policy Start Date'].dt.year\n",
    "\n",
    "train['Month'] = train['Policy Start Date'].dt.month\n",
    "test['Month'] = test['Policy Start Date'].dt.month\n",
    "\n",
    "train['Month_cos'] = np.cos(2 * np.pi * train['Month'] / 12)\n",
    "train['Month_sin'] = np.sin(2 * np.pi * train['Month'] / 12)\n",
    "\n",
    "test['Month_cos'] = np.cos(2 * np.pi * test['Month'] / 12)\n",
    "test['Month_sin'] = np.sin(2 * np.pi * test['Month'] / 12)\n",
    "\n",
    "train['Day'] = train['Policy Start Date'].dt.day\n",
    "test['Day'] = test['Policy Start Date'].dt.day\n",
    "\n",
    "train['Day_cos'] = np.cos(2 * np.pi * train['Day'] / 30)\n",
    "train['Day_sin'] = np.sin(2 * np.pi * train['Day'] / 30)\n",
    "\n",
    "test['Day_cos'] = np.cos(2 * np.pi * test['Day'] / 30)\n",
    "test['Day_sin'] = np.sin(2 * np.pi * test['Day'] / 30)\n",
    "\n",
    "train['Group'] = (train['Year'] - 2020) * 48 + train['Month'] * 4 + train['Day'] // 7\n",
    "test['Group'] = (test['Year'] - 2020) * 48 + test['Month'] * 4 + test['Day'] // 7\n",
    "\n",
    "train.drop(columns=['Day','Month','Policy Start Date'], inplace=True)\n",
    "test.drop(columns=['Day','Month','Policy Start Date' ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the categorical columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = le.fit_transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Premium Amount']\n",
    "X = train.drop(columns=['Premium Amount'])\n",
    "\n",
    "X_train, X_ref, y_train, y_ref = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsLe(y_true, y_pred):\n",
    "    y_pred = np.maximum(y_pred, 1e-6)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# SEED = 42\n",
    "# n_splits = 5\n",
    "\n",
    "# def TrainML(params, e_stop=50):\n",
    "#     kfold = RepeatedKFold(n_splits=n_splits, n_repeats=1, random_state=SEED)\n",
    "#     train_rmse_scores = []\n",
    "#     val_rmse_scores = []\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "#         X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#         y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#         y_train_log = np.log1p(y_train)\n",
    "#         y_val_log = np.log1p(y_val)\n",
    "        \n",
    "#         callbacks = [lgb.early_stopping(stopping_rounds=e_stop, verbose=False)]\n",
    "#         model = lgb.LGBMRegressor(**params, random_state=SEED, verbose=-1, n_jobs=-1)\n",
    "#         model.fit(X_train, y_train_log, \n",
    "#                   eval_set=[(X_val, y_val_log)], \n",
    "#                   eval_metric='rmse', \n",
    "#                   callbacks=callbacks)\n",
    "        \n",
    "#         y_train_log_pred = model.predict(X_train)\n",
    "#         y_val_log_pred = model.predict(X_val)\n",
    "        \n",
    "#         y_train_pred = np.expm1(y_train_log_pred)\n",
    "#         y_val_pred = np.expm1(y_val_log_pred)\n",
    "        \n",
    "#         train_rmse = rmsLe(y_train, y_train_pred)\n",
    "#         val_rmse = rmsLe(y_val, y_val_pred)\n",
    "\n",
    "#         train_rmse_scores.append(train_rmse)\n",
    "#         val_rmse_scores.append(val_rmse)\n",
    "\n",
    "#     mean_train_rmse = np.mean(train_rmse_scores)\n",
    "#     mean_val_rmse = np.mean(val_rmse_scores)\n",
    "\n",
    "#     print(\"\\n Final Mean Scores:\")\n",
    "#     print(f\" Mean Train RMSLE: {mean_train_rmse:.4f}\")\n",
    "#     print(f\" Mean Validation RMSLE: {mean_val_rmse:.4f}\")\n",
    "\n",
    "#     return mean_train_rmse, mean_val_rmse\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "#         'device': 'gpu'\n",
    "#     }\n",
    "\n",
    "#     mean_train_rmse, mean_val_rmse = TrainML(params)\n",
    "#     trial.set_user_attr('train_rmse', mean_train_rmse)\n",
    "#     trial.set_user_attr('val_rmse', mean_val_rmse)\n",
    "#     return mean_val_rmse\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    " - LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 700, 'learning_rate': 0.006002641114390338, 'num_leaves': 42, 'max_depth': 15, 'min_child_samples': 72, \n",
    "               'subsample': 0.7200344333898133, 'colsample_bytree': 0.6986521753741868, 'reg_alpha': 0.0013223748424590466, \n",
    "               'reg_lambda': 0.40630468090624405}\n",
    "\n",
    "# #Training LGBMRegressor Model - It is known for being fast and efficient in training, especially on large datasets.\n",
    "# lgbm_model = LGBMRegressor(**best_params, verbose=-1, n_jobs=-1)\n",
    "# lgbm_model.fit(X_train, np.log1p(y_train))#this log is recommended when the distribution is skewed\n",
    "\n",
    "# #Make predictions\n",
    "# test_preds = lgbm_model.predict(X_ref)\n",
    "# test_preds = np.expm1(test_preds)\n",
    "\n",
    "# error_lgbm = rmsLe(y_ref, test_preds)\n",
    "\n",
    "# #Visualising the importance of each field of the dataset in determing the Premium Amount\n",
    "# importances = lgbm_model.feature_importances_\n",
    "# norm_importances = importances / np.linalg.norm(importances)\n",
    "# features = X.columns\n",
    "# importance_df = pd.DataFrame({'Feature': features, 'Importance': norm_importances})\n",
    "# importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "# plt.title('Error = {:.5f}'.format(error_lgbm))\n",
    "# plt.show()\n",
    "# #Error 1.04967"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# best_params_xgb = {'n_estimators': 700, 'learning_rate': 0.006002641114390338, 'max_depth': 15, 'min_child_weight': 72,\n",
    "#                    'subsample': 0.7200344333898133, 'colsample_bytree': 0.6986521753741868, 'reg_alpha': 0.0013223748424590466,\n",
    "#                     'reg_lambda': 0.40630468090624405}\n",
    "\n",
    "# xgb_model = XGBRegressor(**best_params_xgb, objective='reg:squarederror', random_state=42)\n",
    "# xgb_model.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "# test_preds = xgb_model.predict(X_ref)\n",
    "# test_preds = np.expm1(test_preds)\n",
    "\n",
    "# error_xgb = rmsLe(y_ref, test_preds)\n",
    "\n",
    "# # sample_submission['Premium Amount'] = test_preds\n",
    "# # sample_submission.to_csv('submission1.csv', index=False)\n",
    "\n",
    "# #Visualising the importance of each field of the dataset in determing the Premium Amount\n",
    "# importances = xgb_model.feature_importances_\n",
    "# norm_importances = importances / np.linalg.norm(importances)\n",
    "# features = X.columns\n",
    "# importance_df = pd.DataFrame({'Feature': features, 'Importance': norm_importances})\n",
    "# importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "# plt.title('Error = {:.5f}'.format(error_xgb))\n",
    "# plt.show()\n",
    "# #Error = 1.05278"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AutoMl with h2o server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h2o\n",
    "# from h2o.automl import H2OAutoML\n",
    "# from sklearn.metrics import mean_squared_log_error\n",
    "# import numpy as np\n",
    "\n",
    "# h2o.init()\n",
    "\n",
    "# data = h2o.import_file(\"./train.csv\")\n",
    "\n",
    "# train, test = data.split_frame(ratios=[0.8], seed=42)\n",
    "\n",
    "# target = data.columns[-1]\n",
    "# features = data.columns[:-1]\n",
    "\n",
    "# aml = H2OAutoML(\n",
    "#     max_models=15,\n",
    "#     max_runtime_secs=600,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# aml.train(x=features, y=target, training_frame=train)\n",
    "# lb = aml.leaderboard\n",
    "# print(lb)\n",
    "\n",
    "# auto_model = aml.leader\n",
    "# y_pred = auto_model.predict(test[features])\n",
    "\n",
    "# y_pred_np = np.array(y_pred.as_data_frame()['predict'])\n",
    "# y_true_np = np.array(test[target].as_data_frame())\n",
    "\n",
    "# error_auto = rmsLe(y_true_np, y_pred_np)\n",
    "# print(\"RMSLE:\", error_auto)\n",
    "# #error 1.12 because the data is not preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# best_params_hgb = {'max_iter': 700, 'learning_rate': 0.006002641114390338, 'max_depth': 15, 'min_samples_leaf': 72,\n",
    "#                    'max_bins': 255, 'l2_regularization': 0.40630468090624405, 'random_state': 42}\n",
    "\n",
    "# hgb_model = HistGradientBoostingRegressor(**best_params_hgb)\n",
    "# hgb_model.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "# test_preds = hgb_model.predict(X_ref)\n",
    "# test_preds = np.expm1(test_preds)\n",
    "\n",
    "# error_hgb = rmsLe(y_ref, test_preds)\n",
    "# print(error_hgb)\n",
    "\n",
    "# #Error = 1.0509144449116223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural Network Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_ref = scaler.transform(X_ref)\n",
    "# y_train_values = y_train.to_numpy()\n",
    "# y_ref_values = y_ref.to_numpy()\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train_values, dtype=torch.float32)\n",
    "# X_ref_tensor = torch.tensor(X_ref, dtype=torch.float32)\n",
    "# y_ref_tensor = torch.tensor(y_ref_values, dtype=torch.float32)\n",
    "\n",
    "# X_train_tensor = torch.nan_to_num(X_train_tensor, nan=0.0)\n",
    "# y_train_tensor = torch.nan_to_num(y_train_tensor, nan=0.0)\n",
    "# X_ref_tensor = torch.nan_to_num(X_ref_tensor, nan=0.0)\n",
    "# y_ref_tensor = torch.nan_to_num(y_ref_tensor, nan=0.0)\n",
    "\n",
    "# # Define the neural network model\n",
    "# class SimpleNN(nn.Module):\n",
    "#     def _init_(self):\n",
    "#         super(SimpleNN, self)._init_()\n",
    "#         self.layer1 = nn.Linear(24, 64)  # 24 input features, 64 nodes in the first hidden layer\n",
    "#         self.layer2 = nn.Linear(64, 32)  # 64 nodes in the second hidden layer\n",
    "#         self.output = nn.Linear(32, 1)  # Output layer (1 output node for regression)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.layer1(x))  # ReLU activation for the first layer\n",
    "#         x = torch.relu(self.layer2(x))  # ReLU activation for the second layer\n",
    "#         x = self.output(x)  # No activation for the output layer\n",
    "#         return x\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = SimpleNN()\n",
    "\n",
    "# criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "# epochs = 200\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     optimizer.zero_grad()  # Zero out the gradients from the previous step\n",
    "    \n",
    "#     # Forward pass\n",
    "#     outputs = model(X_train_tensor)\n",
    "#     loss = criterion(outputs.squeeze(), y_train_tensor)  # Squeeze to match target dimensions\n",
    "    \n",
    "#     # Backward pass\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Gradient clipping\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "#     # Optimization step\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Update learning rate\n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Print loss every 10 epochs\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_ref_tensor).squeeze()  # No gradients needed for evaluation\n",
    "\n",
    "# # Calculate the Mean Squared Error on the test set\n",
    "# mse = rmsLe(predictions, y_ref_tensor)\n",
    "# print(f'Test Loss (MSE): {mse.item():.4f}')\n",
    "# #Error 1.1635"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# best_params_ada = {'n_estimators': 700, 'learning_rate': 0.006002641114390338, 'random_state': 42}\n",
    "\n",
    "# columns_to_drop = [\n",
    "#     \"Age\", \"Insurance Duration\", \"Day_sin\", \"Education Level\", \"Policy Type\", \n",
    "#     \"Day_cos\", \"Month_sin\", \"Month_cos\", \"Vehicle Age\", \"Number of Dependents\", \n",
    "#     \"Marital Status\", \"Exercise Frequency\", \"Occupation\", \"Location\", \n",
    "#     \"Property Type\", \"Smoking Status\", \"Gender\"\n",
    "# ]\n",
    "\n",
    "# X_ref = X_ref.drop(columns=columns_to_drop)\n",
    "# X_train = X_train.drop(columns=columns_to_drop)\n",
    "\n",
    "# # Crearea și antrenarea modelului AdaBoost Regressor cu DecisionTreeRegressor ca estimator de bază\n",
    "# base_estimator = DecisionTreeRegressor(max_depth=4)\n",
    "# model = AdaBoostRegressor(estimator=base_estimator, **best_params_ada)\n",
    "# model.fit(X_train, np.log1p(y_train))  # Transformare log pentru o distribuție skewed\n",
    "\n",
    "# # Predicția\n",
    "# y_pred = model.predict(X_ref)\n",
    "# y_pred = np.expm1(y_pred)  # Inversarea transformării logaritmice\n",
    "\n",
    "# # Calcularea erorii\n",
    "# error_adaboost = rmsLe(y_ref, y_pred)\n",
    "# #Error 1.0939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# # from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# # Crearea modelului SVM\n",
    "# params = {\n",
    "#     'kernel': 'linear',  # Tipul de kernel utilizat\n",
    "#     'C': 1.0  # Parametrul de regularizare\n",
    "# }\n",
    "# model = SVC(**params)\n",
    "\n",
    "# # Antrenarea modelului\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predicția\n",
    "# y_pred = model.predict(X_ref)\n",
    "\n",
    "# # Calcularea erorii \n",
    "# error_rmsle = rmsLe(y_ref, y_pred)\n",
    "\n",
    "# # Importanța caracteristicilor prin permutation importance\n",
    "# perm_importance = permutation_importance(model, X_ref, y_ref, scoring='accuracy', n_repeats=30, random_state=42)\n",
    "# importances = perm_importance.importances_mean\n",
    "\n",
    "# # Normalizarea importanțelor\n",
    "# norm_importances = importances / np.linalg.norm(importances)\n",
    "# features = X.columns\n",
    "# importance_df = pd.DataFrame({'Feature': features, 'Importance': norm_importances})\n",
    "# importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# # Vizualizarea importanței caracteristicilor\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "# plt.title('Error = {:.5f}')\n",
    "# plt.show()\n",
    "# #Error 1.212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best model is LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LGBMRegressor(**best_params, verbose=-1, n_jobs=-1)\n",
    "best_model.fit(X, np.log1p(y))\n",
    "\n",
    "test_preds = best_model.predict(test)\n",
    "test_preds = np.expm1(test_preds)\n",
    "\n",
    "sample_submission['Premium Amount'] = test_preds\n",
    "sample_submission.to_csv('submission1.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
